{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfee4883-e47e-4503-906c-b35266e7c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, SVR\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb2e32-91dc-4d0d-a61b-244508898c75",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e620d603-4a34-4deb-8429-38076c106793",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/consecutive_games_df.pkl', 'rb') as handle:\n",
    "    full_df = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "76fa1b59-8696-4506-9043-55589650010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = LinearRegression\n",
    "CLF_MODEL = LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ac30ea-00cb-410f-b0fd-bb4e3cdbb590",
   "metadata": {},
   "source": [
    "# Propensity and IPW\n",
    "We'll use Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "eb1558d2-a727-435e-8133-3e8f03820ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['time_between_games'] = (full_df['next_date'] - full_df['prev_date']).apply(lambda x: x.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "532293e1-46f6-4f82-a977-34223e015762",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['lost_prev_game'] = full_df['prev_goal_diff'] < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "639ebb83-546a-47fd-b558-1898324aeddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['row_team', 'league_id', 'prev_match_api_id', 'prev_date', 'prev_rival_team_api_id', 'next_match_api_id', \n",
    "             'next_date', 'next_rival_team_api_id', 'next_rival_team_goal', 'prev_goal_diff',\n",
    "             'next_row_team_goal', 'prev_row_team_goal', 'prev_rival_team_goal']\n",
    "bet_cols = [col for col in full_df.columns if '_bet_' in col]\n",
    "T = 'lost_prev_game'\n",
    "Y = 'next_mean_bet_row_team'\n",
    "# T = 'prev_home'\n",
    "# Y = 'prev_goal_diff'\n",
    "CATEGORICAL_COLS = ['season']\n",
    "X_COLS = [col for col in full_df.columns if col not in drop_cols + bet_cols + [Y, T] + CATEGORICAL_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "8ad3e1fd-0d79-435b-a3c0-ea88098af9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['row_team_wins',\n",
       " 'row_team_draws',\n",
       " 'row_team_league_points',\n",
       " 'row_team_season_goals',\n",
       " 'prev_home',\n",
       " 'prev_row_team_min_squad_rating',\n",
       " 'prev_rival_team_min_squad_rating',\n",
       " 'prev_row_team_max_squad_rating',\n",
       " 'prev_rival_team_max_squad_rating',\n",
       " 'prev_row_team_mean_squad_rating',\n",
       " 'prev_rival_team_mean_squad_rating',\n",
       " 'prev_row_team_std_squad_rating',\n",
       " 'prev_rival_team_std_squad_rating',\n",
       " 'prev_row_team_median_squad_rating',\n",
       " 'prev_rival_team_median_squad_rating',\n",
       " 'prev_rival_wins',\n",
       " 'prev_rival_draws',\n",
       " 'prev_rival_league_points',\n",
       " 'prev_rival_season_goals',\n",
       " 'next_home',\n",
       " 'next_goal_diff',\n",
       " 'next_row_team_min_squad_rating',\n",
       " 'next_rival_team_min_squad_rating',\n",
       " 'next_row_team_max_squad_rating',\n",
       " 'next_rival_team_max_squad_rating',\n",
       " 'next_row_team_mean_squad_rating',\n",
       " 'next_rival_team_mean_squad_rating',\n",
       " 'next_row_team_std_squad_rating',\n",
       " 'next_rival_team_std_squad_rating',\n",
       " 'next_row_team_median_squad_rating',\n",
       " 'next_rival_team_median_squad_rating',\n",
       " 'next_rival_wins',\n",
       " 'next_rival_draws',\n",
       " 'next_rival_league_points',\n",
       " 'next_rival_season_goals',\n",
       " 'time_between_games']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "a2adc35a-6573-4059-8ddc-10f1404b56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = full_df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "e7fee560-e38e-4fea-9d67-29e4b931fc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    df_categorical = data[CATEGORICAL_COLS]\n",
    "    dummies = pd.get_dummies(df_categorical)\n",
    "    data[dummies.columns] = dummies\n",
    "    return data.drop(columns=df_categorical.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "9449edfd-77b8-41bd-a3fd-4b8680b5d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propensity(data, model=CLF_MODEL):\n",
    "    # Preprocess\n",
    "    # Use standard scaling\n",
    "    # Apply Logistic Regression on X, predict t\n",
    "    # Return model's probabilities\n",
    "    data = data.copy()\n",
    "    data = prepare_data(data)\n",
    "    \n",
    "    X = data[X_COLS]\n",
    "    t = data[T]\n",
    "    \n",
    "    steps = [('scaler', StandardScaler()),\n",
    "             ('clf', model(random_state=0))]\n",
    "    pipe_lr = Pipeline(steps)\n",
    "    clf = pipe_lr.fit(X, t)\n",
    "    \n",
    "    return clf.predict_proba(X)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8384797c-b253-4fb4-a830-e2601be67b15",
   "metadata": {},
   "source": [
    "### Propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "a386f143-cbac-4071-a3a8-f7ffbfb68378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Propensity: \n",
      "[0.17743464 0.56997592 0.24451984 ... 0.2134336  0.22166519 0.50090744]\n"
     ]
    }
   ],
   "source": [
    "prop = propensity(df)\n",
    "print(f'Data Propensity: \\n{prop}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "277f5e32-9254-4484-b063-7afa7d4e8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame([prop1, prop2], index=['data1', 'data2']).to_csv('models_propensity.csv', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a5053-5785-4a62-96d6-380f31282837",
   "metadata": {},
   "source": [
    "### IPW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "7f1232f5-5dd7-44ac-89cf-1c048130b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def att(data, prop):\n",
    "    # Calculate IPW formula (tutorial 8)\n",
    "    left = ((data[T] * data[Y]).sum() / data[T].sum())\n",
    "    up = (((1 - data[T]) * data[Y]) * (prop / (1 - prop))).sum()\n",
    "    down = ((1 - data[T]) * (prop / (1 - prop))).sum()\n",
    "    return left - (up / down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "ee08032b-2751-4e04-a65b-fe1e705cdf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipw(data, model=CLF_MODEL):\n",
    "    prop = propensity(data, model)\n",
    "    att_hat_ipw = att(data, prop)\n",
    "    return att_hat_ipw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "2396573a-3abc-401c-8429-fc8f869561f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_hat_ipw = 0.11936874037251632\n"
     ]
    }
   ],
   "source": [
    "att_hat_ipw = ipw(df)\n",
    "print(f'{att_hat_ipw = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3b7c6-68c9-4ed6-9c75-c8d21f69e23d",
   "metadata": {},
   "source": [
    "# S-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "ab828411-8ce7-4464-8cdf-e9a0c8819007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "class InteractionsNoT(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.poly = PolynomialFeatures(interaction_only=True,include_bias = False)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        dummies_columns = CATEGORICAL_COLS\n",
    "        self.poly.fit(X.drop(columns=[T] + dummies_columns), y)  # Perform interactions on X without T\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        dummies_columns = CATEGORICAL_COLS\n",
    "        dummies_values = X[dummies_columns]\n",
    "        t = X[T]\n",
    "        X = self.poly.transform(X.drop(columns=[T] + dummies_columns))\n",
    "        X = np.column_stack((X, dummies_values, t))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "7a4bf8be-e382-4063-b96b-eb98347f1f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_learner_model(data, model=MODEL):\n",
    "    # Preprocess\n",
    "    # Use standard scaling\n",
    "    # Apply Linear Regression on X with t, predict y\n",
    "    # Return model\n",
    "    data = data.copy()\n",
    "    data = prepare_data(data)\n",
    "    X = data.drop(columns=[Y])\n",
    "    y = data[Y]\n",
    "\n",
    "    steps = [('scaler', StandardScaler()),\n",
    "             ('clf', model())]\n",
    "    pipe_lr = Pipeline(steps)\n",
    "    clf = pipe_lr.fit(X, y)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "6920bc2a-0d32-4945-b327-2245854c4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_learner_model_with_interactions(data, model=MODEL):\n",
    "    # Preprocess\n",
    "    # Use standard scaling\n",
    "    # Apply Linear Regression on X (+interactions) with t, predict y\n",
    "    # Return model\n",
    "    data = data.copy()\n",
    "    data = prepare_data(data)\n",
    "    X = data.drop(columns=[Y])\n",
    "    y = data[Y]\n",
    "\n",
    "    steps = [('interactions', InteractionsNoT()),\n",
    "             ('scaler', StandardScaler()),\n",
    "             ('clf', model())]\n",
    "    pipe_lr = Pipeline(steps)\n",
    "    clf = pipe_lr.fit(X, y)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "32041451-baa8-4fa5-90aa-4ba94f39f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_att_s_learner(data, clf):\n",
    "    # Use S-learner (tutorial 8)\n",
    "    data = data.copy()\n",
    "    data = prepare_data(data).drop(columns=[Y])\n",
    "    f1 = data[data[T]==1]\n",
    "    f0 = data[data[T]==1].copy()\n",
    "    f0[T] = 0\n",
    "    \n",
    "    return (clf.predict(f1) - clf.predict(f0)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "dccf0f55-16ac-4bb4-99bb-877052d13679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_learner(data, use_interactions=False, model=MODEL):\n",
    "    # Call S-learner with or without interactions\n",
    "    clf = s_learner_model(data, model=model) if not use_interactions else s_learner_model_with_interactions(data, model=model)\n",
    "    return calc_att_s_learner(data, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c38fc8-c3ec-47cf-bcc9-82ca3f2de86e",
   "metadata": {},
   "source": [
    "### No interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "5c3219dc-2fea-4211-b0e4-60b7117be252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_hat_s_learner = 0.025316439531624615\n"
     ]
    }
   ],
   "source": [
    "att_hat_s_learner = s_learner(df)\n",
    "print(f'{att_hat_s_learner = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b900cdfa-6b92-4c06-ac78-ddd362f116b2",
   "metadata": {},
   "source": [
    "### With interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ebeb51c-460a-4342-929b-edf0ac2758da",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['season'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m att_hat_s_learner_inters \u001b[38;5;241m=\u001b[39m \u001b[43ms_learner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00matt_hat_s_learner_inters \u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36ms_learner\u001b[1;34m(data, use_interactions, model)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21ms_learner\u001b[39m(data, use_interactions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, model\u001b[38;5;241m=\u001b[39mMODEL):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Call S-learner with or without interactions\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     clf \u001b[38;5;241m=\u001b[39m s_learner_model(data, model\u001b[38;5;241m=\u001b[39mmodel) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_interactions \u001b[38;5;28;01melse\u001b[39;00m \u001b[43ms_learner_model_with_interactions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m calc_att_s_learner(data, clf)\n",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36ms_learner_model_with_interactions\u001b[1;34m(data, model)\u001b[0m\n\u001b[0;32m     11\u001b[0m steps \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minteractions\u001b[39m\u001b[38;5;124m'\u001b[39m, InteractionsNoT()),\n\u001b[0;32m     12\u001b[0m          (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()),\n\u001b[0;32m     13\u001b[0m          (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m'\u001b[39m, model())]\n\u001b[0;32m     14\u001b[0m pipe_lr \u001b[38;5;241m=\u001b[39m Pipeline(steps)\n\u001b[1;32m---> 15\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mpipe_lr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clf\n",
      "File \u001b[1;32md:\\technion\\8th_semester\\causal\\venv\\lib\\site-packages\\sklearn\\pipeline.py:378\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    377\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 378\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\technion\\8th_semester\\causal\\venv\\lib\\site-packages\\sklearn\\pipeline.py:336\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    334\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 336\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    337\u001b[0m     cloned_transformer,\n\u001b[0;32m    338\u001b[0m     X,\n\u001b[0;32m    339\u001b[0m     y,\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    341\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    342\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    344\u001b[0m )\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32md:\\technion\\8th_semester\\causal\\venv\\lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\technion\\8th_semester\\causal\\venv\\lib\\site-packages\\sklearn\\pipeline.py:870\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 870\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    872\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32md:\\technion\\8th_semester\\causal\\venv\\lib\\site-packages\\sklearn\\base.py:870\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36mInteractionsNoT.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m      9\u001b[0m     dummies_columns \u001b[38;5;241m=\u001b[39m CATEGORICAL_COLS\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoly\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdummies_columns\u001b[49m\u001b[43m)\u001b[49m, y)  \u001b[38;5;66;03m# Perform interactions on X without T\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\technion\\8th_semester\\causal\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\technion\\8th_semester\\causal\\venv\\lib\\site-packages\\pandas\\core\\frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4806\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   4807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   4808\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4815\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   4816\u001b[0m ):\n\u001b[0;32m   4817\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4818\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   4819\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4952\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   4953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4956\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4960\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4961\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4962\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\technion\\8th_semester\\causal\\venv\\lib\\site-packages\\pandas\\core\\generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4267\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32md:\\technion\\8th_semester\\causal\\venv\\lib\\site-packages\\pandas\\core\\generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[0;32m   4309\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4311\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4312\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4314\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\technion\\8th_semester\\causal\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6644\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6644\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6645\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['season'] not found in axis\""
     ]
    }
   ],
   "source": [
    "att_hat_s_learner_inters = s_learner(df, True)\n",
    "print(f'{att_hat_s_learner_inters = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57407033-401d-4362-9b50-a0cef8a4446f",
   "metadata": {},
   "source": [
    "# T-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "94ed0435-0359-4ee9-b7c5-364f453168ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_learner_model(data, t, model=MODEL):\n",
    "    # Preprocess and take only data with treatment t\n",
    "    # Use standard scaling\n",
    "    # Apply Linear Regression on X with t, predict y  # TODO: with T or without T in moedel?\n",
    "    # Return model\n",
    "    data = data.copy()\n",
    "    data = prepare_data(data)\n",
    "    data = data[data[T] == t]\n",
    "    X = data.drop(columns=[Y, T])\n",
    "    y = data[Y]\n",
    "\n",
    "    steps = [('scaler', StandardScaler()),\n",
    "             ('clf', model())]\n",
    "    pipe_lr = Pipeline(steps)\n",
    "    clf = pipe_lr.fit(X, y)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "353ee780-53b9-4265-b692-b2d9ca53f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_att_t_learner(data, clf0, clf1):\n",
    "    # Use T-learner (tutorial 8)\n",
    "    data = data.copy()\n",
    "    data = prepare_data(data)\n",
    "    d_treated = data[data[T] == 1].drop(columns=[Y, T])\n",
    "    \n",
    "    return (clf1.predict(d_treated) - clf0.predict(d_treated)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "2bbfe2d4-3597-45ef-921b-7330cb7be50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_learner(data, model=MODEL):\n",
    "    # Call S-learner with or without interactions\n",
    "    clf0 = t_learner_model(data, 0, model=model)\n",
    "    clf1 = t_learner_model(data, 1, model=model)\n",
    "    return calc_att_t_learner(data, clf0, clf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "63ff247d-9a6d-452f-996b-9efb3a6d09a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_hat_t_learner = 0.03292420809889714\n"
     ]
    }
   ],
   "source": [
    "att_hat_t_learner = t_learner(df)\n",
    "print(f'{att_hat_t_learner = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c045cb3-974f-42db-85fa-af9dbc54ba2a",
   "metadata": {},
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dfecdade-6372-4abb-8a99-008ffbf86c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "0d3ba277-d28a-40bd-9649-27721e2e99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_1NN(data):\n",
    "    # Preprocess\n",
    "    # Prepare data pairs\n",
    "    # Calculate ITEs (lesson 3)\n",
    "    # Calculate Average\n",
    "    data = prepare_data(data)\n",
    "    \n",
    "    data_T0 = data[data[T] == 0]\n",
    "    data_T0 = data_T0.drop(columns=[T])\n",
    "    data_T1 = data[data[T] == 1]\n",
    "    data_T1 = data_T1.drop(columns=[T])\n",
    "    \n",
    "    ITEs = []\n",
    "    dists = cdist(np.array(data_T1.drop(columns=[Y])).astype(float), np.array(data_T0.drop(columns=[Y])).astype(float))\n",
    "    for i, j in enumerate(dists.argmin(axis=1)):\n",
    "        ITE_i = data_T1[Y].iloc[i] - data_T0[Y].iloc[j]\n",
    "        ITEs.append(ITE_i)\n",
    "    \n",
    "    return np.mean(ITEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "298caa20-fdd8-43bb-8962-6bc28e0d14e3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m att_hat_matching_1NN \u001b[38;5;241m=\u001b[39m \u001b[43mmatching_1NN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00matt_hat_matching_1NN \u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [69]\u001b[0m, in \u001b[0;36mmatching_1NN\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     14\u001b[0m dists \u001b[38;5;241m=\u001b[39m cdist(np\u001b[38;5;241m.\u001b[39marray(data_T1\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[Y]))\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m), np\u001b[38;5;241m.\u001b[39marray(data_T0\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[Y]))\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dists\u001b[38;5;241m.\u001b[39margmin(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m---> 16\u001b[0m     ITE_i \u001b[38;5;241m=\u001b[39m \u001b[43mdata_T1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mY\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata_T0\u001b[49m\u001b[43m[\u001b[49m\u001b[43mY\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     17\u001b[0m     ITEs\u001b[38;5;241m.\u001b[39mappend(ITE_i)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(ITEs)\n",
      "\u001b[1;31mTypeError\u001b[0m: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead."
     ]
    }
   ],
   "source": [
    "att_hat_matching_1NN = matching_1NN(df)\n",
    "print(f'{att_hat_matching_1NN = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "e43a5e4a-d534-42c1-a57f-39e81fc7263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_general(data, k=10):\n",
    "    # Preprocess\n",
    "    # Prepare data near groups\n",
    "    # Calculate ITEs (lesson 3)\n",
    "    # Calculate Average\n",
    "    data = prepare_data(data)\n",
    "    \n",
    "    data_T0 = data[data[T] == 0]\n",
    "    data_T0 = data_T0.drop(columns=[T])\n",
    "    data_T1 = data[data[T] == 1]\n",
    "    data_T1 = data_T1.drop(columns=[T])\n",
    "    \n",
    "    ITEs = []\n",
    "    i_dict = {}  # J(i)\n",
    "    dists = cdist(np.array(data_T1.drop(columns=[Y])).astype(float), np.array(data_T0.drop(columns=[Y])).astype(float))\n",
    "    k_nearest = np.argpartition(dists, k)[:,:k]\n",
    "    for i, close_inds in enumerate(k_nearest):\n",
    "        ITE_i = data_T1[Y].iloc[i] - data_T0[Y].iloc[close_inds].mean()\n",
    "        ITEs.append(ITE_i)\n",
    "    \n",
    "    return np.mean(ITEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "8d77ca49-0c71-46f3-ba7b-5048103b2125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_hat_matching_general = -0.03810090104517261\n"
     ]
    }
   ],
   "source": [
    "att_hat_matching_general = matching_general(df)\n",
    "print(f'{att_hat_matching_general = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f498dc-7664-495b-aa56-7105bfd9f1fd",
   "metadata": {},
   "source": [
    "# Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f4cfc2f4-04af-47f1-80ab-3a948f2fdc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: \n",
      "(<class 'sklearn.linear_model._logistic.LogisticRegression'>, <class 'sklearn.linear_model._base.LinearRegression'>)\n",
      "ipw\n",
      "s\n",
      "t\n",
      "Models: \n",
      "(<class 'sklearn.ensemble._forest.RandomForestClassifier'>, <class 'sklearn.ensemble._forest.RandomForestRegressor'>)\n",
      "ipw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivsf\\AppData\\Local\\Temp\\ipykernel_28364\\3947841571.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  up = (((1 - data[T]) * data[Y]) * (prop / (1 - prop))).sum()\n",
      "C:\\Users\\nivsf\\AppData\\Local\\Temp\\ipykernel_28364\\3947841571.py:5: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  down = ((1 - data[T]) * (prop / (1 - prop))).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n",
      "t\n",
      "Models: \n",
      "(<class 'sklearn.tree._classes.DecisionTreeClassifier'>, <class 'sklearn.tree._classes.DecisionTreeRegressor'>)\n",
      "ipw\n",
      "s\n",
      "t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nivsf\\AppData\\Local\\Temp\\ipykernel_28364\\3947841571.py:4: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  up = (((1 - data[T]) * data[Y]) * (prop / (1 - prop))).sum()\n",
      "C:\\Users\\nivsf\\AppData\\Local\\Temp\\ipykernel_28364\\3947841571.py:5: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  down = ((1 - data[T]) * (prop / (1 - prop))).sum()\n"
     ]
    }
   ],
   "source": [
    "scores1, scores2 = [], []\n",
    "\n",
    "for clf_model, lin_model in zip([LogisticRegression, RandomForestClassifier, DecisionTreeClassifier], \n",
    "                                [LinearRegression, RandomForestRegressor, DecisionTreeRegressor]):\n",
    "    print(f'Models: \\n{clf_model, lin_model}')\n",
    "    print('ipw')\n",
    "    ipw1 = ipw(df, model=clf_model)\n",
    "    \n",
    "    print('s')\n",
    "    s1 = s_learner(df, model=lin_model)\n",
    "    \n",
    "    print('t')\n",
    "    t1 = t_learner(df, model=lin_model)\n",
    "    \n",
    "    scores1 += [ipw1, s1, t1]\n",
    "\n",
    "scores1 += [att_hat_matching_1NN, att_hat_matching_general]\n",
    "\n",
    "att1_hat_comp = np.nanmean(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "354a5df1-18f5-4bdd-aa10-9c3597785435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.021499890800913697"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att1_hat_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b67ccb3-1f66-42e0-8747-bf8feb00b358",
   "metadata": {},
   "source": [
    "## Save all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc6df1d5-17f0-4ac2-b656-15ce51b34278",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame([\n",
    "    [att1_hat_ipw, att2_hat_ipw], \n",
    "    [att1_hat_s_learner , att2_hat_s_learner], \n",
    "    [att1_hat_t_learner , att2_hat_t_learner], \n",
    "    [att1_hat_matching_general, att2_hat_matching_general],\n",
    "    [att1_hat_comp, att2_hat_comp]\n",
    "], \n",
    "columns = ['data1', 'data2'])\n",
    "final['Type'] = np.arange(1, len(final) + 1)\n",
    "final[['Type', 'data1', 'data2']].to_csv('ATT_results.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "910cac31-6473-45fc-b217-5f0e40c60cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5822b975-f7c9-4ce4-a1ba-271a490dc2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a ZipFile object\n",
    "zipObj = ZipFile('205460686_316361641.zip', 'w')\n",
    "\n",
    "# Add multiple files to the zip\n",
    "zipObj.write('models_propensity.csv')\n",
    "zipObj.write('ATT_results.csv')\n",
    "\n",
    "# close the Zip File\n",
    "zipObj.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
